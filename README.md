# Neural-network-and-fuzzy-logic

**Assignment 1:** 
1. Implement the **linear regression algorithm** to estimate the weight parameters for the feature matrix (X) and the class label vector (y). (a) Plot the cost function vs the number of iterations. (b) Plot the cost function (J) vs w1 and w2 in a contour or 3D surf graph (w= [w0 w1 w2]). Please use the dataset “data.xlsx”. (Use for or while loop for the implementation)
2. Implement stochastic gradient descent for the linear regression problem in question number 1. (a) Plot the cost function vs the number of iterations. (b) Plot the cost function vs w1 and w2. (Please use the dataset “data.xlsx”). (Use for or while loop for the implementation)
3. Implement the **ridge regression** problem by considering both batch gradient descent and stochastic gradient descent. (a) Plot the cost function vs the number of iterations for both the cases. (b) Plot the cost function (J) vs w1 and w2 in a contour or 3D surf graph for both the cases. (Please use the dataset “data.xlsx”). (Use for or while loop for the implementation)
4. Implement the **Vectorized linear regression** problem to evaluate the weight parameters for question number 1. Compare the weight parameters with the weights obtained using both gradient descent and stochastic gradient descent based algorithms. (Please use the dataset “data.xlsx”).
5. Implement **Least angle regression** to estimate the weight parameters for the feature matrix (X) and the class label vector (y) by considering both gradient descent and stochastic gradient descent based algorithms. (Please use the dataset “data.xlsx”). (Use for or while loop for the implementation).
6. Implement **K-means clustering** based unsupervised learning algorithm for the dataset (“data2.xlsx”). Plot the estimated class labels vs features. Use the number of clusters as K=2.
7. Implement the **logistic regression** algorithm for the binary classification using the dataset (“data3.xlsx”). Divide the dataset into training and testing using hold-out cross- validation technique with 60 % of instances as training and the remaining 40% as testing. Evaluate the accuracy, sensitivity and specificity values for the binary classifier.
8. Implement the **multiclass logistic regression** algorithm using both “One VS All” and “One VS One” multiclass coding techniques. Evaluate the performance of the multiclass classifier using individual class accuracy and overall accuracy measures. Use the hold-out cross-validation approach (60% training and 40% testing) for the selection of training and test instances. (Please use the dataset “data4.xlsx”)
9. Evaluate the performance of multiclass **logistic regression classifier using 5-fold cross- validation approach**. Evaluate the individual class accuracy and overall accuracy measures for the multiclass classifier along each fold. (Please use the dataset “data4.xlsx”)
10. Use the likelihood ratio test (LRT) for the binary classification using the dataset (“data3.xlsx”). Divide the dataset into training and testing using hold-out cross-validation technique with 60 % of instances as training and the remaining 40% as testing. Evaluate the accuracy, sensitivity and specificity values for the binary classifier.
11. Implement the **Maximum a posteriori (MAP) decision rule for multiclass classification** task. Use the hold-out cross-validation approach (70% training and 30% testing) for the selection of training and test instances. (Please use the dataset “data4.xlsx”).
12. Implement the **Maximum likelihood (ML) decision rule for multiclass classification task**. Use the hold-out cross-validation approach (70% training and 30% testing) for the selection of training and test instances. (Please use the dataset “data4.xlsx”).

**Assigment 2:**
1. Implement AND, OR, NOT, ANDNOT, NAND, NOR, XOR and XNOR gates using Hebbian learning rule based perceptron algorithm. Evaluate the optimal weight parameters for each logic gate. Plot the cost function vs number of iterations for each logic gate.
2. Implement the **multilayer feed forward neural network (MFNN)** (three layers: input layer, hidden layer and output layer) for the classification using the loop based implementation algorithm. (Please use dataset.xlsx and hold out cross validation with 70% as training and 30 % as testing instances selection). Plot the cost function vs number of iterations. Evaluate the optimal number of hidden layer neurons based on grid search approach with the maximum accuracy for the test instances as the objective function.)
3. Implement the **MFNN (4 layers: input, hidden layer 1, hidden layer 2 and output layer) for the classification using the vectorization based algorithm**. (Please use dataset.xlsx and hold out cross validation with 70% as training and 30 % as testing instances selection). Plot the cost function vs number of iterations. Evaluate the optimal number of hidden layer neurons based on grid search approach with the maximum accuracy for the test instances as the objective function.)
4. Implement the **radial basis function neural network (RBFN)** for the classification. (Please use dataset.xlsx and hold out cross validation with 70% as training and 30 % as testing instances selection). Evaluate the optimal number of hidden layer neurons based on grid search approach with the maximum accuracy for the test instances as the objective function.)
5. Implement a **deep neural network** as shown below. You can use back propagation algorithm, non-linear basis function evaluation and **RBFNN** weight extraction approach for evaluating optimal weights from input to hidden layers. (Please use dataset.xlsx and hold out cross validation with 70% as training and 30 % as testing instances selection).

**Input-FC1-RBFNN-Output**

**Assignment 3:**
1. The dataset in ‘data_for_cnn.mat’ consists of **1000 ECG signals** and each row corresponds to one ECG signal. The class label for each **ECG signal** is given in ‘class_label. mat’ file. Implement the 1D convolutional neural network with **BPCNN** as the learning algorithm for the evaluation of optimal weight matrices in FC layers and optimal kernels or filters in convolution layer. The network consists of one convolutional layer, one pooling layer and two fully connected (FC) layers. The network flow is given by

**Input-Convolution Layer-Pooling layer-FC1-FC2-Output**

Consider the square loss function as cost function in the output layer. You can consider 20 hidden neurons in the FC2 layer. In the pooling layer, you can use average pooling with down-sampling factor as 2. (For implementation of the **BPCNN** algorithm, please refer to the class notes or slides).
